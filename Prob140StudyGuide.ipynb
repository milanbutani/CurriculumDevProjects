{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I may not have everything from the midterm scope covered on this study guide. Just be aware. http://prob140.org/midterm_contents/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "**Outcome space**: refers to all possible outcomes in an experiment; denoted $\\Omega$\n",
    "<br>\n",
    "<br>\n",
    "**Event**: denoted by capital letter of alphabet, such as $A$ and $B$; **event is a subset of the outcome space**\n",
    "<br>\n",
    "<br>\n",
    "**Outcome**: any element of the $\\Omega$; denoted $\\omega$\n",
    "<br>\n",
    "<br>\n",
    "**Counting:** counting number of sequences \n",
    "*when order matters:* is a combination; for example: number of sequences with k heads out of n is $n\\choose{k}$\n",
    "\n",
    "\n",
    "## Finding Probabilities:\n",
    "When the outcome space is uniform, meaning all outcomes have an equal chance of occuring, then probability of an event $A$ is\n",
    "<p style=\"text-align: center;\">$P(A) = \\frac{|A|}{|\\Omega|}$</p>\n",
    "The number of elements that are defined to satisfy condition of event A divided by the total number of elements in the outcome space. \n",
    "\n",
    "<br>\n",
    "\n",
    "### Strategies:\n",
    "#### Complement\n",
    "When the probability is directly hard to find, or there are many terms in the probability, use the complement rule.\n",
    "\n",
    "**Complement Rule:** $P(A) = 1 - P(A^c)$\n",
    "\n",
    "**Example**: Find the probability of getting a $2, 3, 4, 5, 6$ in a roll of die. *This is easy to find without complement.*\n",
    "\n",
    "**Answer**: $P(2, 3, 4, 5, 6) = 1 - P(1)$. Probability of getting a $2,3, 4, 5, 6$ in a roll of a die is the same as not getting a $1$.\n",
    "\n",
    "The example given is bad, since it's easy to compute this without the complement, but it becomes useful for more complicated probabilities, think **probability of a collision among N people**.\n",
    "\n",
    "**Example**: P(all four colors appear) = 1 - P(at least one color doesn't appear)\n",
    "\n",
    "#### Conditional Probabilty/Division Rule:\n",
    "When you want to find the probability of an event while conditioning on something else. \n",
    "\n",
    "<p style=\"text-align: center;\"> $ P(B | A) = \\frac{P(B, A)}{P(A)}$\n",
    "    \n",
    "This is reducing the outcome space. When conditioning over a given event, you are reducing the outcomes to those conditions. And then you want to find a set of outcomes that satisfy another condition to find the conditional probability. \n",
    "\n",
    "**Example**: Find the probability that the card drawn is a heart given that is red:\n",
    "\n",
    "This is: <p style=\"text-align: center;\"> $ P(\\text{card is heart | card is red})$ = $\\frac{P(\\text{card is red and heart})}{P(\\text{card is red})}$.\n",
    "    \n",
    "First you imagine you have a red card, and then you want to find the chance of getting a heart. Getting a red card is reducing the outcome space to 26 cards, and then further conditioning on getting a heart is getting 13 of those 26 cards. The **answer** is $\\frac{1}{2}$.\n",
    "\n",
    "#### Multiplication Rule:\n",
    "Follows from the division rule:\n",
    "<p style=\"text-align: center;\">$ P(B | A) \\cdot P(A) = P(B, A)$\n",
    "    \n",
    "#### Independence/Dependence/Mutually Exclusive/Inclusion-Exclusion:\n",
    "\n",
    "**Mutually Exclusive**: When two events $A$ and $B$ are mutually exclusive, only one of them can happen at any given time. This means there is a dependence relationship between the two events, as only one can happen. In other words, if we know one event happened, we know the other didnt.\n",
    "\n",
    "Formally: <p style=\"text-align: center;\">$ P(A, B) = 0$ if $A$ and $B$ are mutually exlusive.\n",
    "\n",
    "**Unions/Intersections:** When A and B are not mutually exclusive both of them can happen at the same time. \n",
    "\n",
    "<p style=\"text-align: center;\">$ P(A, B) = P(A) * P(B)$ but this requires that $A$ and $B$ are independent events. \n",
    "    \n",
    "<p style=\"text-align: center;\"> When $A$ and $B$ are dependent, $ P(A, B) = P(A) * P(B|A) $.\n",
    "    \n",
    "**Addition Rule:**   \n",
    "<p style=\"text-align: center;\"> $ P(A \\cup B) = P(A) + P(B) - P(A, B)$.\n",
    "    \n",
    "However, when $A$ and $B$ are mutually exclusive:<p style=\"text-align: center;\"> $ P(A \\cup B) = P(A) + P(B)$ because $P(A, B) = 0$ since only one of them can happen at a time.\n",
    "    \n",
    "**Bayes Rule:** is an application of both the multiplication and division rules\n",
    "<p style=\"text-align: center;\"> $ P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B}$\n",
    "    \n",
    "**Boole's Inequality:** When the extend of the intersection between events is unknown, using inequalities can be a good measure to approximate probabilities. \n",
    "<br>\n",
    "<br>\n",
    "Say you have $N$ events labeled $A_1, A_2, \\dots A_N$:\n",
    "<br>\n",
    "<br>\n",
    "When those $N$ events are mutually independent, this means there is no overlap between any two events. What is the probability of one of the events occuring?\n",
    "<p style=\"text-align: center;\"> $ P(\\bigcup\\limits_{i=1}^{\\infty} A_{i}) ≤  \\sum_{i=1}^{\\infty} P(A_i)$\n",
    "\n",
    "This tells us that the chance of one of them is no more than the sum of all of them, which makes sense as an upper bound. \n",
    "<br>\n",
    "<br>\n",
    "What about the lower bound? Need to think worst case for the minimum, when all the events are bounded inside another:\n",
    "<br>\n",
    "<p style=\"text-align: center;\"> $ max(P(A_i) : 1 ≤ i ≤ N) ≤ P(\\bigcup\\limits_{i=1}^{\\infty} A_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions\n",
    "\n",
    "**A distribution on a random variable is given by:**\n",
    "1. the values a variable can take on\n",
    "2. the probabilities of the random value takes on a certain value.\n",
    "\n",
    "### Random Variables \n",
    "<br>\n",
    "\n",
    "**Bernoulli:** with parameter $p$ denoting the chance of a success; is how an indicator random variable is distributed.\n",
    "<br>\n",
    "Say $X$ is a Bernoulli random variable with parameter $p$:\n",
    "<p style=\"text-align: center;\"> $P(X = 1) = p$ </p>\n",
    "<p style=\"text-align: center;\"> $P(X = 0) = 1 - p$ </p>\n",
    "<p style=\"text-align: center;\"> $E[X] = p$ </p>\n",
    "\n",
    "**Takes on values 0, 1**\n",
    "\n",
    "**Binomial**: with parameters $p$, $n$  denoting the chance of a success, and number of independent and identically distributed trials; is a distrbution for the count of successes in a trial, can be though of as **n** $Bernoulli(p)$ random variables, one for each individual in the sample.\n",
    "<br>\n",
    "Say $X$ is a Binomial random variable with parameters $p$ and $n$:\n",
    "<p style=\"text-align: center;\"> $P(X = k)$ = $n\\choose k$ $\\cdot p^k \\cdot (1 - p)^{n - k} $</p>\n",
    "<p style=\"text-align: center;\"> $E[X] = np$ </p>\n",
    "\n",
    "Looks a lot like a normal distribution, but isn't centered. There are a lot of possible values, but the number of probable values is low.\n",
    "\n",
    "**Takes on values 0, 1, 2, $\\dots$ n**\n",
    "\n",
    "**Poisson**: with parameter $\\mu$, which is the mean of the distribution. \n",
    "<br>\n",
    "Say $X$ is a Poisson random variable with parameters $\\lambda$:\n",
    "<p style=\"text-align: center;\"> $P(X = k)$ = $e^{-\\lambda} \\cdot \\frac{\\lambda^k}{k!}$</p>\n",
    "<p style=\"text-align: center;\"> $E[X] = \\lambda$ </p>\n",
    "\n",
    "**Takes on values 0, 1, 2, $\\dots$ n**\n",
    "\n",
    "**Geometric:** with parameter $p$, which is the probability of a success. \n",
    "<br>\n",
    "Say $X$ is a Geometric random variable with parameter $p$:\n",
    "<p style=\"text-align: center;\"> $P(X = 1) = p$ </p>\n",
    "<p style=\"text-align: center;\"> $P(X = 10) = p \\cdot 1 - p^{9}$ </p>\n",
    "<p style=\"text-align: center;\"> $P(X = k) = p \\cdot 1 - p^{k - 1}$ </p>\n",
    "<p style=\"text-align: center;\"> $E[X] = \\frac{1}{p}$ </p>\n",
    "\n",
    "**Takes on values 1, 2, 3, $\\dots$ n.** These values denote the number of times you need to wait for a success. Meaning if $ X = 1 $, then that means there were zero failures and one success.\n",
    "\n",
    "**Hypergeometric:** has parameter $N$, $G$, $n$, where $N$ is the size of the population, $G$ is the number of good elements, and $n$ is the size of the sample you are drawing from the population.\n",
    "<br>\n",
    "Say $X$ is a Hypergeometric random variable with parameters $N$, $G$, $n$:\n",
    "<p style=\"text-align: center;\"> $P(X = 1) = \\frac{{G \\choose 1} \\cdot {N - G \\choose n - 1}}{N \\choose n}$ </p>\n",
    "<p style=\"text-align: center;\"> $P(X = k) = \\frac{{G \\choose k} \\cdot {N - G \\choose n - k}}{N \\choose n}$ </p>\n",
    "<p style=\"text-align: center;\"> $E[X] = n \\cdot \\frac{G}{N}$ </p>\n",
    "\n",
    "**Takes on values 0, 1, 2 $\\dots$ G**\n",
    "\n",
    "**Multinomial:** very similar t binomial except there are more than two classes. All trials or picks from a population have constant population, meaning that probability of picking a class doesn't change over time. \n",
    "<br>\n",
    "Say you have three classes, $n_1$, $n_2$, $n_3$ with probabilities $p_1$, $p_2$, $p_3$. Say there are $N$ people in the population you are choosing from.\n",
    "<br>\n",
    "<p style=\"text-align: center;\"> $P(3\\text{ from $n_1$, $4$ from $n_2$ , $5$ from $n_3$} ) = \\frac{N!}{3! \\cdot 4! \\cdot 5!} \\cdot p_1^{n_1} \\cdot p_2^{n_2} \\cdot p_3^{n_3}$\n",
    "<br>\n",
    "    \n",
    "**What happens to the classes and their probabilities when the number of trials goes from fixed to a Poisson number of trials?**\n",
    "\n",
    "When number of trials is fixed, the number in each class is Binomial:\n",
    "<p style=\"text-align: center;\"> Say number of trials is $N$.</p>\n",
    "<p style=\"text-align: center;\"> Probability of k successes is $P(X = k) = {n\\choose k}$ $\\cdot p^k \\cdot (1 - p)^{n - k} $</p>\n",
    "<p style=\"text-align: center;\"> Which ends up being the same as the probability of n - k failures, which is $P(\\text{failures} = n - k) = {n\\choose n - k}$ $\\cdot p^k \\cdot (1 - p)^{n - k} $</p>\n",
    "\n",
    "**Verify that the two above are the same**\n",
    "\n",
    "When number of trials is Poisson, the number in each class is no longer Binomial; it is Poisson:\n",
    "<p style=\"text-align: center;\"> Say number of trials is $Poission(\\mu)$.</p>\n",
    "<p style=\"text-align: center;\"> Probability of k successes is $P(X = k) = e^{-p\\mu} \\cdot \\frac{p\\mu^k}{k!}$</p>\n",
    "<p style=\"text-align: center;\"> Which is not the same as the probability of n - k failures, which is $P(\\text{failures} = n - k) = e^{-(1-p)\\mu} \\cdot \\frac{(1-p)\\mu^k}{k!}$</p>\n",
    "<p style=\"text-align: center;\"> $P(\\text{k successes, n - k failures}) = P(\\text{k successes}) \\cdot P(\\text{n - k failures})$\n",
    "    \n",
    "**When we Poissonize the Binomial, the probailites of a class becomes independent. That last line drives the point home.**\n",
    "\n",
    "**The same thing happens to the Multinomial. When Poissonizing that, the probability of each class becomes independent of the other.**\n",
    "\n",
    "### Equality and Equality in Distribution: \n",
    "**Equal**: when two random variables on the same outcome space have values that are same for every outcome in the space. \n",
    "<p style=\"text-align: center;\"> Denoted: $X = Y$\n",
    "<p style=\"text-align: center;\"> $X(\\omega) = Y(\\omega)$ for all $\\omega \\in \\Omega$\n",
    "    \n",
    "**Equality in Distirbution:** two random variables have the same distribution if they have the same probability distribution–same set of possible values and probabilities on those values. \n",
    "<p style=\"text-align: center;\"> Denoted: $X \\overset{d}{=} Y$\n",
    "   \n",
    "**Equality implies equal in distribution, but not the other way around.**\n",
    "<p style=\"text-align: center;\"> Meaning $X = Y \\implies X \\overset{d}{=} Y$\n",
    "    \n",
    "### Joint, Marginal, Conditional, Independence:\n",
    "**Joint Distribution:** of $X$ and $Y$ consists of all $P(X = x, Y = y)$ for all $(x, y)$ pairs which range over all values of $(X, Y)$.\n",
    "\n",
    "**Sum of a joint distribution must be equal to one.*\n",
    "\n",
    "**Marginal Distirbution:** represents the distirbution of one variable from a joint distribution; sums appear in the margins of a distribution. \n",
    "\n",
    "**Sum of a marginal distribution must be equal to one.**\n",
    "\n",
    "**Conditional Distribution:** can be used to understand the relation between ttwo variables; gives insight into the dependence between two variables. If we know \n",
    "\n",
    "**A conditional distribution must sum to one.**\n",
    "\n",
    "**Dependence:** If knowing the value of one of the two random variables in a joint distribution changes our opinion on the values for the other random variable, then the two are dependent. If any one conditional distribution is different from the rest, then there is dependence.\n",
    "\n",
    "**Independence:** If knowing the value of one of the two random variables in a joint distribution doesn't change our opinion on the values for the other random variable, then there is independence. All conditional distributions must be the same as each other, Meaning the conditional distirbution doesn't differ from the marginal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation:\n",
    "**Expectation** is a long-run average. Found by summing all the products of values and their probabiltiies:\n",
    "<p style=\"text-align: center;\"> $E[X] = \\sum_{} x \\cdot P(X = x)$ for all $x$ values $X$ can take on.\n",
    "    \n",
    "Using **Tail-Sum Formula** to find expectation of non-negative integer valued variable:\n",
    "<p style=\"text-align: center;\"> $E[X] = P(X = 1) + 2P(X = 2) + 3P(X = 3) + \\dots $\n",
    "<p style=\"text-align: center;\"> $E[X] = \\sum\\limits_{k = 0}^{\\infty}  P(X > k)$\n",
    "\n",
    "### Properties:\n",
    "**Additivity:** holds for depdentent and independent random variable:\n",
    "<p style=\"text-align: center;\"> $E[X + Y] = E[X] + E[Y]$\n",
    "    \n",
    "**Linearity of Expectation:** expectation of a linear transformation of X is the linear transformation of the expectation of X:\n",
    "<p style=\"text-align: center;\"> $E[c] = c$, where $c$ is a constant\n",
    "<p style=\"text-align: center;\"> $E[X + 1] = E[X] + E[1]$\n",
    "<p style=\"text-align: center;\"> $E[aX + b] = aE[X] + E[b]$    \n",
    "    \n",
    "**Need to manually compute $E[X^2]$**.\n",
    "\n",
    "**Will add conditional expectation and waiting times later**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain\n",
    "A Markov Chain is defined by its **state space** and transition matrix $\\boldsymbol{P}$\n",
    "\n",
    "**Irreducible:** means you can go from one state to any other state; also means you can go from one state back to itself while hitting every other state. \n",
    "\n",
    "**Aperiodic:** means you can get back to that chain in any number of steps, and without following any regular pattern. \n",
    "\n",
    "**State space:** refers to all the states; a list of all the states.\n",
    "\n",
    "$\\boldsymbol{P}$: transition probability matrix; all rows add to one, since they are each a probability distribution. The $(i, j)$ element describes going from state $i$ to $j$. Matrix size is $S \\times S$, where $S$ is the number of states in the chain.\n",
    "\n",
    "**All chains are aperiodic and irreducible in the class.**\n",
    "\n",
    "**Stationary Distribtion or Steady-State Distribution:** is a row vector that **adds to one** and reprents the long run proportion of time the chain spends its time in a certain state, given it is irreducible and aperiodic. **A chain only has a steady state distribution if it is aperiod and irreducible.** Solved by finding $\\pi$ that is non-zero and solves $\\pi = \\pi \\cdot \\boldsymbol{P}$.\n",
    "\n",
    "**Birth and Death chain:** A chain in which any state can either has to go up one or down one, but may or may not stay at the same state. In other words, it doesn't matter if it stays in the same state space, but it must go up or down for it to be a birth and death chain. \n",
    "\n",
    "**Detailed Balance:** An easy way to solve for the steady-state distribution. All birth and death chains have steady-state distributions. \n",
    "\n",
    "**Will add Code Breaking later**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximations\n",
    "\n",
    "**Log approximation**: $log(1 + x) \\approx x$ when x is small.\n",
    "\n",
    "**e approximation:** $e^x \\approx 1 + x$ for small x.\n",
    "\n",
    "**Taylor expansion of log(1 + x):** $$\\sum_{k=1}^{\\infty} -1^{k + 1} * \\frac{x^k}{k}$$\n",
    "\n",
    "**Geometric series $e^x$** =  $$\\sum_{k=0}^{\\infty}\\frac{x^k}{k!}$$\n",
    "\n",
    "**Approximate the Binomial distribution using a Poisson distribution:** \n",
    "<p style=\"text-align: center;\">Binomial($n$, $p$) $\\approx$ Poisson($np$) when $n$ is large and $p$ is small.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
