{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2 Study Guide\n",
    "#### I may not have covered everything in scope, just be aware.\n",
    "\n",
    "# Chapter 12, Lecture 12:\n",
    "\n",
    "## Standard Deviation\n",
    "\n",
    "**Devitation from the mean** refers to how far a value is from its mean (expected value). \n",
    "<p style=\"text-align: center;\"> $X - \\mu_X$ </p>\n",
    "\n",
    "If you apply Linearity of Expectation to the formula above, you will get that the **Expectation of the Deviation is $0$.** \n",
    "\n",
    "Since the Expected Deviation is 0, it is hard to understand the true deviation, which is why deviations squared are important, since they capture the magnitude of the deviation for each value. \n",
    "\n",
    "**Standard Deviation is the root mean square of deviations from the mean:**\n",
    "\n",
    "<p style=\"text-align: center;\"> $SD(X) = \\sigma_X = \\sqrt{E((X - \\mu_X)^2)}$ </p>\n",
    "\n",
    "The **variance**, aka $E((X - \\mu_X)^2)$, needs to be square rooted because its units is hard to understand. The **SD** has the same units as $X$ and $E(X)$.\n",
    "\n",
    "#### Finding Standard Deviation:\n",
    "1. Find $E(X)$.\n",
    "2. FInd $E(X^2)$.\n",
    "3. Caluclate $Var(X) = E(X^2) - E(X)^2$.\n",
    "4. Square root $Var(X)$ to find $SD(X)$.\n",
    "\n",
    "#### Properties of SD:\n",
    "1. Shifting $X$ to the left or right doesn't affect the $SD(X)$ because all values are shifted similarly. \n",
    "2. Multiplying $X$ by a factor $c$ will make the $SD(c \\cdot X) = |c| \\cdot SD(X)$. The proof for this is in how the $Var(X)$ is different from the $Var(cX)$ in a specific way.\n",
    "3. $SD(-X) = SD(X)$ by the property above.\n",
    "\n",
    "#### SDs of Common Random Variables (know how to compute these):\n",
    "1. SD of an Indicator $I$:\n",
    "<p style=\"text-align: center;\"> $SD(I) = \\sqrt{p(p-1)}$ </p>\n",
    "2. SD of Uniform $U$:\n",
    "<p style=\"text-align: center;\"> $SD(U) = \\sqrt{\\frac{n^2 - 1}{12}}$ </p>\n",
    "3. SD of Poisson($\\mu$) $X$:\n",
    "<p style=\"text-align: center;\"> $SD(X) = \\sqrt{\\mu}$ </p>\n",
    "\n",
    "#### Choosing Unbiased Estimator:\n",
    "Choose an estimatro that has a lower SD and is unbiased. An estimator is unbiased when its mean is the same as the mean of the sampling distribution. \n",
    "\n",
    "## Tail Bounds:\n",
    "\n",
    "### Markov's Inequality:\n",
    "Can be used to find the tail probability that a value for the random variable is greater than some value:\n",
    "\n",
    "**Usable when:** the expected value of the random variable is known and **the random variable is non-negative**. \n",
    "<p style=\"text-align: center;\"> $P(X ≥ c) ≤ \\frac{E(X)}{c}$ </p>\n",
    "\n",
    "This tail bound is weak; it is only based off the expectation, and sometimes it is stupid to use. \n",
    "\n",
    "We can say that **probability that random variable is at least k times the mean is $\\frac{1}{k}$**:\n",
    "<p style=\"text-align: center;\"> $P(X ≥ k\\mu_X) ≤ \\frac{E(X)}{k\\mu_X} = \\frac{1}{k}$ </p>\n",
    "\n",
    "### Chebyshev Inequality:\n",
    "Can be used to find the tail probability that a value is within a certain raing from the mean:\n",
    "\n",
    "**Usable when:** the expected value and the standard deviation (or variance) are known; the interval must be symmetric about the mean; **random variable must be non-negative**\n",
    "<p style=\"text-align: center;\"> $P(|X - \\mu_X| ≥ c) ≤ \\frac{Var(X)}{c^2} = \\frac{SD(X)^2}{c^2}$ </p>\n",
    "\n",
    "This tail bound is stronger than Markov's, which makes sense since there is more information required to apply Chebyshev as compared to Markov. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13, Lecture 13:\n",
    "\n",
    "## Variances\n",
    "**Variance of a sum is NOT the sum of the variances**. (proof in chapter 13)\n",
    "\n",
    "If $S$ = $X$ + $Y$, then:\n",
    "<p style=\"text-align: center;\"> $Var(S) = Var(X) + Var(Y) + 2E((X - \\mu_X)(Y - \\mu_Y))$ </p>\n",
    "\n",
    "Also written using $D_X$ and $D_Y$ to denote deviations for $X$ and $Y$:\n",
    "<p style=\"text-align: center;\"> $Var(S) = Var(X) + Var(Y) + 2E(D_X \\cdot D_Y)$ </p>\n",
    "\n",
    "**Covariance** of $X$ ad $Y$ is the expectation of the product of their deviations: \n",
    "<p style=\"text-align: center;\"> $Cov(X, Y) = Cov(Y, X) = E(D_X \\cdot D_Y)$ </p>\n",
    "\n",
    "#### Properties of Covariance:\n",
    "1. $Cov(X, c) = 0$ where $C$ is a constant. \n",
    "2. $Cov(X, X) = Var(X)$ \n",
    "3. $Cov(X, Y) = Cov(Y, X)$\n",
    "4. $Cov(X, Y) = E(XY) - \\mu_X\\mu_Y$, which can be used to prove that $Cov(X, X) = Var(X)$\n",
    "5. $Cov(X + Y, Z) = Cov(X, Z) + Cov(Y, Z)$\n",
    "6. **Biliniearity**: $Cov(aX , bY) = abCov(X, Y)$\n",
    "7. $Cov(X, Y) = 0$ when $X$ and $Y$ are independent.\n",
    "\n",
    "#### Sum of IID Random Variables:\n",
    "When you add up IID random variables, then the variance of that sum is just the sum of all the variances of individual random variables:\n",
    "<p style=\"text-align: center;\"> $Var(X_1 + X_2 + \\dots + X_n) = Var(X_1) + Var(X_2) + \\dots + Var(X_N)$ </p>\n",
    "\n",
    "This is because all the covariances for each pair of the random variables will be equal to $0$. \n",
    "\n",
    "#### Variance of Binomial:\n",
    "$Var(X)$ where $X$ is binomial with parameters $(n, p)$ is $np(1-p) = npq$. This is found by calculating the variances of the $n$ indicators for the $n$ trials chosen with probability $p$. (chapter 13.2)\n",
    "\n",
    "#### Sum of Simple Random Variables:\n",
    "When you add up elements in a simple random sample, they will be dependent on each other. This is because you sample without replacement. Then the variance of that sum is the sum of all the variances of individual random variables and the sum of all covariancs for each $j≠k$ pair.\n",
    "\n",
    "This time all the covariances for each $i≠j$ pair of the random variables will not be equal to $0$ since they are dependent.\n",
    "\n",
    "#### Variance of Binomial:\n",
    "$Var(X)$ where $X$ is hypergeometric with parameters $n, G, N$ is $npq \\cdot \\frac{N - n}{N - 1}$. This is found by calculating the variances of the $n$ indicators for the $n$ chosen individuals in the sample. (chapter 13.3)\n",
    "\n",
    "#### Variance of Sample Sum in general:\n",
    "$Var(X) = n\\sigma^2\\frac{N - n}{N - 1}$.\n",
    "Follows the same steps as for the binomial and hypergeometric sum variances above.\n",
    "\n",
    "**Read 13.4, it's pretty cool!**\n",
    "\n",
    "#### Strategy for finding the Variance of Sums:\n",
    "1. Identify whether the random variables that build the sum are dependent or independent. \n",
    "2. You will have to find the variance of each random variable in the sum and the covariances for each $i≠j$ pair. \n",
    "3. Find the variance of each random variable using the steps for variance. \n",
    "4. If the random variables are dependent, then find the covariance by caluclating the $E(X_1X_2)$ and then subtracting $\\mu_X\\mu_Y$ from it.\n",
    "5. Add the sum of variance term and the sum of covariance terms that you found earlier. Make sure you multiple by relevant numbers. For instance, for variance of sum of $n$ random variables be sure to add $n\\cdot Var(X_1)$ and $n(n-1)\\cdot Cov(X_1, X_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14, Lecture 14\n",
    "\n",
    "## Probability Generating Functions:\n",
    "A probability generating function is defined by the polynomial:\n",
    "<p style=\"text-align: center;\"> $G_X(s) = \\sum_{k=0}^{N} p_ks^k$</p>\n",
    "\n",
    "<p style=\"text-align: center;\"> $G_X(s) = p_o + p_1s + p_2s^2 + p_3s^3 + \\dots + p_Ns^N$</p>\n",
    "\n",
    "This means that $P(X = k) = p_k$, which is when the coefficient is $s^k$ on the PGF.\n",
    "\n",
    "Say you have a random variable $X$, and you want to find the expectation of $s^X$. You would apply the funcion to each $x$ value $X$ can take on and weight by probability:\n",
    "<p style=\"text-align: center;\"> $ p_os^0 + p_1s^1 + p_2s^2 + p_3s^3 + \\dots + p_Ns^N = \\sum_{k=0}^{N} p_ks^k$</p>\n",
    "\n",
    "Which is just the PFG of $X$, $G_X(s)$. This means that\n",
    "<p style=\"text-align: center;\"> $G_X(s) = E(s^X)$ </p>\n",
    "\n",
    "If you were trying to find $G_{X + Y}(s)$ when $X$ and $Y$ are independent, you would find the expecation of $s$ raised to the $X + Y$ power:\n",
    "<p style=\"text-align: center;\"> $G_{X + Y}(s) = E(s^{X + Y}) = E(s^X \\cdot s^Y) = E(s^X)\\cdot E(s^Y) = G_{X}(s) \\cdot G_{Y}(s)$</p>\n",
    "\n",
    "#### PGF of Sum of IID Sample:\n",
    "<p style=\"text-align: center;\"> $G_{S_n}(s) = (G_{X_1}(s))^n$</p>\n",
    "\n",
    "The formula for $G_{S_n}(s)$ above is found similarly to how the formula for $G_{X + Y}(s)$ was found. \n",
    "\n",
    "Remember that the $P(S_n = k) = $ the coefficient for the $s^k$ term in the PGF.\n",
    "\n",
    "## CLT\n",
    "\n",
    "#### Standard Normal Conversion:\n",
    "Given a random variable $X$ with mean $\\mu_X$ and SD $\\sigma_X$ it can be converted in standard units, which meausres how far the value is from the mean in standard units:\n",
    "<p style=\"text-align: center;\"> $Z = \\frac{X - \\mu_X}{\\sigma_X}$</p>\n",
    "\n",
    "## CLT\n",
    "**CLT states that the sample sum or average will be roughly normal regardless of how the individual random variables that build the sum are distributed.**\n",
    "\n",
    "Say the sum $S_n$ is defined to be a sum of a bunch of random variable $X$ values with some random ass distribution. You will be picking values of $X$ which will be near the mean for $X$, since that is most likely. All values you picked in your sample that build to the sum will be normal, because you are picking from relatively near the mean of $X$, which has a pretty high chance of happening each time. Since you keep picking from near the mean of $X$, the $S_n$ will be normal. Same holds if you changed \"sample sum\" to \"sample average.\"\n",
    "\n",
    "#### Standard Normal CDF:\n",
    "Is a fucntion $\\Phi(z)$ that finds the probability of everything to the left of the input point. Notice the input vakue is a normalized score or z-score.\n",
    "\n",
    "#### How do samples change as sample size increases?\n",
    "Sample Sum: Sample sum's standard deviation grows by a factor of $\\sqrt{\\text{growth factor}}$ as the size of n grows. Sample expectation will always grow larger, but this isn't defined by any function. (chapter 14.6)\n",
    "\n",
    "Sample Mean: Expecataion of sample means always remains the same regardless of the growth of the sample size. However the standard deviation of the sample mean will decrease as the size increases by a factor of $\\frac{1}{\\sqrt{\\text{growth rate}}}$. This means that as the sample gets bigger, the sample mean becomes a better and better estimator of the population mean, which makes sense, cause you'll start choosing the whole populationa as the sample size increases. (chapter 14.6)\n",
    "\n",
    "## Confidence Intervals:\n",
    "To find a confidence interval, you need to use the inverse of the standard normal CDF, which will give you a point at which there is a certain probability. ```stats.norm.ppf```\n",
    "\n",
    "They are interpreted as there is a $C\\%$ chance that the random interval contains will contain the parameter you are estimating, where $C$ is the confidence level. \n",
    "\n",
    "Confidence Interval = $\\mu ± z_{\\lambda}\\frac{\\sigma}{\\sqrt(n)}$\n",
    "\n",
    "$z_\\lambda$ is the z-score for relevant range for your confidence interval.\n",
    "\n",
    "$\\mu$ is the sample average, and $\\sigma$ is the sample standard deviation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
