{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "<BR>\n",
    "Data100 Textbook: https://www.textbook.ds100.org/ch/10/modeling_intro.html\n",
    "<BR>\n",
    "Foundations of Statistical Inference slides by Sandrine Dudoit:       http://www.ds100.org/sp19/assets/lectures/lec12/inference.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:\n",
    "- **model**: an idealized representation of a system; allows us to make predictions based on data used to create the mode; predictions will almost alwyas make incorrect predictions; *any model we create is an approximation of a real-world process*\n",
    "- **population parameter**: a single value hat represents some attirbute of the large number of values within the population; representative of the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss: \n",
    "- **loss function**: a function that takes in as input a value of $\\Theta$ and points in our dataset; outputs a single number the is used to select the best value of $\\Theta$ we can\n",
    "- measures how well a given value of $\\Theta$ fits the data\n",
    "    - $\\Theta$ denotes the population parameter\n",
    "    - by convention, loss function outputs lower values (for loss) for preferable values of $\\Theta$ and larger values (for loss) for values of $\\Theta$ that minimize loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions:\n",
    "\n",
    "## Mean Sqaures Error (MSE):\n",
    "1. Select a value of $\\Theta$\n",
    "2. For each value in the dataset, take the squared difference between the value and $\\Theta$\n",
    "3. compute the final loss by taking the avergae of each sqaured difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python that looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(theta, values):\n",
    "    return np.mean((y_vals - theta) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE penalizes values that are far form the center of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimizing the MSE function: \n",
    "Since our loss function is a function of theta, we can take the partial derivative of it in terms of theta and then find the minimizing value. \n",
    "\n",
    "**Observation**: Minimizing value of the MSE is the average of the data values for the constant model. There is a single value of $\\Theta$ that gives us the least MSE no matter what the data is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE):\n",
    "1. Select a value of $\\Theta$\n",
    "2. For each value in the dataset, take the absolue difference between the value and $\\Theta$\n",
    "3. compute the final loss by taking the avergae of each absolute difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(theta, values):\n",
    "    return np.mean(np.abs(theta - values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimizing the MAE function:\n",
    "Since our loss function is a function of theta, we can take the partial derivative of it in terms of theta and then find the minimizing value. When doing this, we observe that to satisfy the derivative for the aboslute value function, we need to pick a value of $\\Theta$ for that has the same number of smaller and larger points. \n",
    "\n",
    "**Observation**: Minimizing value of the MAE is the median of the data points, when we have an odd number of points. When we have an even number of points, the mloss is minimized when $\\Theta$ is any value in between the two cenral points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE  vs MSE: \n",
    "- Since the MSE has sqaured terms it will be more sensitive to outliers, and this makes sense because the mean is more sensitive to outliers than the median is. \n",
    "- MAE is less sensitive to outlier just like how the median is compared to MSE and the mean.\n",
    "- MAE can have multiple $\\Theta$ values that minimize the loss, but the MSE will always have one $\\Theta$ value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber Loss: MAE + MSE\n",
    "**Huber Loss**: combines both the MSE and the MAE that is differentiable and robust to outliers; behaves likes the MSE for $\\Theta$ that are close to the minimizing value and switching to the MSE for $\\Theta$ that are far from the minimizing value.\n",
    "\n",
    "It has an additional parameter **$\\alpha$** that is set to the point where the Huber loss transitions from MSE to MAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python this (roughly) looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(theta, alpha, values):\n",
    "    total_sum = 0\n",
    "    for point in values:\n",
    "        if np.abs(point - theta) <= alpha:\n",
    "            total_sum += ((point - theta) ** 2)/2\n",
    "        else:\n",
    "            total_sum += alpha*(np.abs(point - theta) - alpha/2)\n",
    "    return total_sum / len(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is hard to differentiate Huber Loss and is tedious, **gradient descent** is used to find the minimizing value of $\\Theta$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
